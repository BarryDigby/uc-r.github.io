---
layout: tutorial
title: Week 2 (October 16-22)
permalink: data_wrangling/week-2
---

 > *"What we have is a data glut."* - Vernon Vinge


Data are being generated by everything around us at all times. Every digital process and social media exchange produces it. Systems, sensors and mobile devices transmit it. Countless databases collect it. Data are arriving from multiple sources at an alarming rate and analysts and organizations are seeking ways to leverage these new sources of information. Consequently, analysts need to understand how to *get* data from these data sources.

Welcome to week 2!  This week we will focus on:

1. The basics of importing tabular and spreadsheet data into R.
2. Getting a basic understanding of the data once its in R. 
3. Advanced importing capabilities such as importing data straight from relational databases (i.e. SQL), web scraping, and importing other statistical software data files (i.e. SPSS, SAS, STATA)

Consequently, this week will give you a strong foundation for the different ways to get your data into R and understanding the basics of your data set. This will prepare you for your first challenge in completing your course project - that of *acquiring* your data!

Below outlines the tutorials that you need to review, and the assignments you need to complete, __*prior*__ to Saturday's class. The skills and functions introduced in these tutorials will be necessary for Saturday's in-class activities.

<hr>

## 1. Basics of Importing, Exporting, and Understanding Your Data

First, learn the basics with the following tutorials and DataCamp assignment.

### Tutorials

1. The basics of importing & exporting data:
    1. Read & work through the [Importing Data tutorial](http://uc-r.github.io/import)
    2. Read & work through the [Exporting Data tutorial](http://uc-r.github.io/exporting)
        - Newbies can focus on the following sections of this tutorial: [exporting to text files](http://uc-r.github.io/exporting#export_text_files) and [exporting to R objects](http://uc-r.github.io/exporting#export_r_objects)
        - Optional: Those more ambitious can also work through [exporting to Excel portion](http://uc-r.github.io/exporting#export_excel_files)
2. Getting a basic understanding of the data once its in R:
    1. Review the codebook: Understanding the source data is crucial to any analysis. A codebook is the documentation that explicitly tells you about the data you are working with and should be the first thing you review before starting any kind of analysis. Read [*Review the Codebook*](codebook) to get a taste of what to look for.
    2. Learn about the data: When first opening a data set it is important to get a basic understanding of the data dimensions (rows and columns), what the data looks like, how many missing values are in the data, and some basic summary statistis such as mean, median, and the range of each variable. Read and work through [*Learn About the Data*](about_the_data) to understand some of the first things you should do with a fresh data set.

### Assignments

- Complete the [Importing Data into R: Part 1](https://www.datacamp.com/groups/data-wrangling-with-r/assignments/9232) DataCamp assignment.

<hr>

## 2. Get to Know Your Data

Next, start learning how to organize your workflow with R Projects and how to create dynamic reporting capabilities with R Markdown by reading and completing the following tutorials and DataCamp assignments.

### Tutorials



<hr>

## Class

Please download this material for Saturday's: &nbsp; <a href="" style="color:black;"><i class="fa fa-cloud-download" style="font-size:1em"></i></a>

See you in class on Saturday!







### Scraping text & tables

- Read & work through the [Importing spreadsheet data files stored online](http://uc-r.github.io/scraping#importing_spreadsheet_data) portion of the [Scraping Data tutorial](http://uc-r.github.io/scraping)
- Optional: For those more ambitious folks, work through the following sections of the [Scraping Data tutorial](http://uc-r.github.io/scraping): [Scraping HTML text](http://uc-r.github.io/scraping#scraping_HTML_text), [Scraping HTML tables](http://uc-r.github.io/scraping#scraping_HTML_tables), and [Leveraging APIs to scrape data](http://uc-r.github.io/scraping#scraping_api)


## Assignment
1. Create a .R script titled “week-2.R” and in this script perform the following exercises:
    - download & import the csv file located at: [https://bradleyboehmke.github.io/public/data/reddit.csv](https://bradleyboehmke.github.io/public/data/reddit.csv)
    - now import the above csv file directly from the url provided (*without* downloading to your local hard drive)
    - download & import the .xlsx file located at: [http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx](http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx)
    - now import the above .xlsx file directly from the url provided (*without* downloading to your local hard drive)
    - go to this University of Dayton webpage [http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm](http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm), scroll down to *Ohio* and import the Cincinnati (OHCINCIN.txt) file


