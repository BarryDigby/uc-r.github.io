---
author: Brad Boehmke
layout: post
comments: true
title: Visualizing ML Models with LIME
---

<img src="https://www.data-imaginist.com/assets/images/lime_logo_small.jpg"  style="float:right; margin: 0px 0px 0px 0px; width: 45%; height: 45%;" />
Machine learning (ML) models are often considered "black boxes" due to their complex inner-workings.  More advanced ML models such as random forests, gradient boosting machines (GBM), artificial neural networks (ANN), among others are typically more accurate for predicting nonlinear, faint, or rare phenomena.  Unfortunately, more accuracy often comes at the expense of interpretability, and interpretability is crucial for business adoption, model documentation, regulatory oversight, and human acceptance and trust.  Luckily, several advancements have been made to aid in interpreting ML models. This [latest tutorial](http://uc-r.github.io/lime) demonstrates how to use the `lime` package, which helps to perform local interpretations of ML models.
