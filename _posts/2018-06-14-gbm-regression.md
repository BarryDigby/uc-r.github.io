---
author: Brad Boehmke
layout: post
comments: true
title: Gradient Boosting Machines
---

<img src="/public/images/analytics/gbm/boosted_stumps.gif"  style="float:right; margin: 2px 5px 0px 20px; width: 30%; height: 30%;" />
Gradient boosting machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions.  Whereas [random forests](random_forests) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous.  When combined, these many weak successive trees produce a powerful "committee" that are often hard to beat with other algorithms.  This [latest tutorial](http://uc-r.github.io/gbm_regression) covers the fundamentals of GBMs for regression problems.
